# Personality Profile Modeling with Transformer VAE

## Overview

This project explores the use of a Variational Autoencoder (VAE) with a Transformer encoder to model personality profiles from questionnaire data. The goal is to learn a latent representation of personality traits that can both reconstruct the original answers and align with predefined categories (inspired by the RIASEC model in the synthetic data).

The current implementation uses synthetically generated data where participant answers are influenced by an underlying 6-dimensional profile (simulating RIASEC categories), and each category has an equal number of associated questions.

## Architecture

The core model (`TransformerVAEStaticHead` located in `model.py`) consists of:

1.  **Embedding Layer:** Encodes answers, question IDs, and question categories into embeddings, which are then concatenated.
2.  **Positional Encoding:** Adds positional information to the combined embeddings.
3.  **Transformer Encoder:** Processes the sequence of embeddings using self-attention to capture relationships between question answers.
4.  **VAE Head:** Maps the pooled output of the Transformer encoder to a latent space defined by a mean (`mu`) and log variance (`logvar`). Reparameterization is used for sampling during training.
5.  **MLP Decoder:** Reconstructs the original questionnaire answers (logits) from the sampled latent vector (`z`).
6.  **Static Prediction Head:** An MLP head that takes the latent mean (`mu`) as input and predicts the underlying 6D static category distribution.

The model is trained using a combined loss function:
*   **Reconstruction Loss:** Binary Cross-Entropy (BCE) between reconstructed answers and original answers.
*   **KL Divergence Loss:** Standard VAE regularization term to encourage the latent space distribution to approximate a standard Gaussian.
*   **Static Prediction Loss:** Mean Squared Error (MSE) between the predicted static distribution and the pre-calculated ground-truth static distribution for each sample.

## Data

The project currently uses synthetic data generated by `data.py`.
*   `NUM_SAMPLES` participants are created.
*   Each participant answers `NUM_QUESTIONS` questions.
*   Questions are pre-assigned to one of `NUM_CATEGORIES` (default 6, like RIASEC), with a balanced number of questions per category.
*   Participant answers are generated based on a latent Dirichlet profile, influencing their probability of answering "yes" to questions associated with specific categories.

## File Structure

```
.
├── config.py           # Hyperparameters and configuration settings
├── data.py             # Data generation and loading functions
├── model.py            # Model definition (PositionalEncoding, TransformerVAEStaticHead)
├── train.py            # Training loop and loss function definition
├── analysis.py         # Evaluation metrics, plotting, and analysis functions
├── main.py             # Main script to orchestrate training and analysis
├── requirements.txt    
└── README.md           # This file
```

## Setup

1.  **Clone the repository:**
    ```bash
    git clone <repository_url>
    cd <repository_directory>
    ```
2.  **Create a virtual environment (recommended):**
    ```bash
    python -m venv .venv
    source .venv/bin/activate  # On Windows use `.venv\Scripts\activate`
    ```
3.  **Install dependencies:**
    You'll need Python 3.x and the following key libraries. You can install them using pip:
    ```bash
    pip install torch numpy scikit-learn matplotlib seaborn pandas plotly tqdm
    ```
    *(Consider creating a `requirements.txt` file for easier installation: `pip freeze > requirements.txt`)*

## Usage

1.  **Configure Hyperparameters:** Modify settings in `config.py` as needed. Key parameters include:
    *   `LEARNING_RATE`, `NUM_EPOCHS`, `BATCH_SIZE`
    *   `BETA`: Weight for KLD loss.
    *   `GAMMA`: Weight for the static prediction loss (MSE). This is crucial for balancing reconstruction vs. alignment with static categories.
    *   `MODEL_SAVE_PATH`: Filename for the saved trained model.
    *   File paths for output plots.
2.  **Run Training and Analysis:**
    ```bash
    python main.py
    ```
    The script will:
    *   Generate synthetic data.
    *   Initialize the model.
    *   Train the model using the combined loss.
    *   Save the trained model's `state_dict` to the path specified in `config.py`.
    *   Perform analysis, calculating metrics (Reconstruction Accuracy, Static Prediction MSE, ARI score between K-Means clusters on `mu` and ground-truth dominant static categories).
    *   Generate and save analysis plots (2D/3D PCA visualizations, correlation heatmaps).
    *   Print a side-by-side comparison of ground-truth static distributions and learned `mu` vectors for a few samples.

## Output

*   **Console Logs:** Progress bars and loss values during training; final metrics (Accuracy, MSE, ARI) during analysis.
*   **Saved Model:** The trained model's parameters saved to the file specified by `MODEL_SAVE_PATH` (e.g., `transformer_vae_static_model.pth`).
*   **Analysis Plots:**
    *   `transformer_vae_static_analysis_2d.png`: 2D PCA of latent space (`mu`) and ground-truth static distributions.
    *   `transformer_vae_static_latent_3d.html`: Interactive 3D PCA plot of the latent space (`mu`).
    *   `transformer_vae_static_static_dist_3d.html`: Interactive 3D PCA plot of the ground-truth static distributions.
    *   `transformer_vae_static_latent_correlation.png`: Correlation heatmap of the latent space dimensions.
    *   `transformer_vae_static_kmeans_centroid_cosine.png`: Cosine similarity heatmap between K-Means centroids found in the latent space.

## Key Libraries

*   PyTorch
*   NumPy
*   Scikit-learn
*   Matplotlib
*   Seaborn
*   Pandas
*   Plotly
*   tqdm