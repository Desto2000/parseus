# Personality Profile Modeling with Transformer VAE

## Overview

This project explores the use of a Variational Autoencoder (VAE) with a Transformer encoder **and an attention-based decoder** to model personality profiles from questionnaire data. The goal is to learn a latent representation of personality traits that can both reconstruct the original answers with high fidelity and align with predefined categories (inspired by the RIASEC model in the synthetic data).

The current implementation uses synthetically generated data where participant answers are influenced by an underlying 6-dimensional profile (simulating RIASEC categories), and each category has an equal number of associated questions.

## Architecture

The core model (`TransformerVAEStaticHead` located in `model.py`) consists of:

1.  **Embedding Layer:** Encodes answers, question IDs, and question categories into embeddings, which are then concatenated.
2.  **Positional Encoding:** Adds positional information to the combined embeddings.
3.  **Transformer Encoder:** Processes the sequence of embeddings using self-attention to capture relationships between question answers, producing a sequence of contextualized output embeddings (`encoder_outputs`).
4.  **VAE Head:** Maps the *pooled* output of the Transformer encoder to a latent space defined by a mean (`mu`) and log variance (`logvar`). Reparameterization is used for sampling the latent vector `z`.
5.  **Static Prediction Head:** An MLP head that takes the latent mean (`mu`) as input and predicts the underlying 6D static category distribution.
6.  **Attention Decoder:**
    *   Uses the sampled latent vector `z` (projected) as a **query**.
    *   Uses the full **`encoder_outputs`** sequence as **keys** and **values**.
    *   A Multi-Head Attention mechanism calculates a `context_vector` representing a weighted sum of `encoder_outputs` relevant to `z`.
    *   An MLP decoder then reconstructs the original questionnaire answers (logits) using the **concatenation of `z` and the `context_vector`** as input.

The model is trained using a combined loss function:
*   **Reconstruction Loss:** Binary Cross-Entropy (BCE) between reconstructed answers and original answers.
*   **KL Divergence Loss:** Standard VAE regularization term.
*   **Static Prediction Loss:** Mean Squared Error (MSE) between the predicted static distribution and the pre-calculated ground-truth static distribution.

## Data

The project currently uses synthetic data generated by `data.py`.
*   `NUM_SAMPLES` participants are created.
*   Each participant answers `NUM_QUESTIONS` questions.
*   Questions are pre-assigned to one of `NUM_CATEGORIES` (default 6, like RIASEC), with a balanced number of questions per category.
*   Participant answers are generated based on a latent Dirichlet profile.

## File Structure

```
.
├── config.py           # Hyperparameters and configuration settings
├── data.py             # Data generation and loading functions
├── model.py            # Model definition (PositionalEncoding, TransformerVAEStaticHead)
├── train.py            # Training loop and loss function definition
├── analysis.py         # Evaluation metrics, plotting, and analysis functions
├── main.py             # Main script to orchestrate training and analysis
├── requirements.txt    # (Optional: Create this to list dependencies)
└── README.md           # This file
```

## Setup

1.  **Clone the repository:**
    ```bash
    git clone <repository_url>
    cd <repository_directory>
    ```
2.  **Create a virtual environment (recommended):**
    ```bash
    python -m venv .venv
    source .venv/bin/activate  # On Windows use `.venv\Scripts\activate`
    ```
3.  **Install dependencies:**
    You'll need Python 3.x and the following key libraries. You can install them using pip:
    ```bash
    pip install -r requirements.txt
    ```
    *(Consider creating a `requirements.txt` file for easier installation: `pip freeze > requirements.txt`)*

## Usage

1.  **Configure Hyperparameters:** Modify settings in `config.py`. Key parameters include `LEARNING_RATE`, `NUM_EPOCHS`, `BETA` (KLD weight), and `GAMMA` (Static MSE weight).
2.  **Run Training and Analysis:**
    ```bash
    python main.py
    ```
    The script trains the model, saves the `state_dict`, performs analysis, and generates output files.

## Results (Attention Decoder Model)

The model incorporating the attention decoder demonstrated excellent performance:

*   **Reconstruction Accuracy:** Achieved near-perfect accuracy (**~99.97%**), significantly outperforming previous iterations thanks to the decoder's ability to attend to the full encoder output sequence.
*   **Latent Space Alignment (ARI):** Maintained strong alignment between the latent space (`mu`) clusters and the ground-truth static categories, with an Adjusted Rand Index (ARI) score of **~0.65**.
*   **Static Distribution Prediction (MSE):** The static prediction head remained highly effective, achieving a very low MSE (**~0.0015**) between its predictions and the target distributions.

This architecture represents the best balance found so far between high-fidelity answer reconstruction and meaningful latent space structure.

## Output

*   **Console Logs:** Training progress and final evaluation metrics.
*   **Saved Model:** Trained model parameters saved to the file specified by `MODEL_SAVE_PATH` in `config.py`.
*   **Analysis Plots:** 2D/3D PCA visualizations, correlation heatmaps, etc., saved to files specified in `config.py`.

## Key Libraries

*   PyTorch
*   NumPy
*   Scikit-learn
*   Matplotlib
*   Seaborn
*   Pandas
*   Plotly
*   tqdm